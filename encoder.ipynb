{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4FKVHhbzgX7"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "from collections import OrderedDict\n",
        "logging.basicConfig(filename='output/example.log', level=logging.INFO)\n",
        "logger = logging.getLogger()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7srxmu3632iy"
      },
      "outputs": [],
      "source": [
        "# Copyright (c) Facebook, Inc. and its affiliates.\n",
        "# All rights reserved.\n",
        "#\n",
        "# This source code is licensed under the license found in the\n",
        "# LICENSE file in the root directory of this source tree.\n",
        "#\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "import math\n",
        "\n",
        "__all__ = [\n",
        "    'resnet50',\n",
        "    'resnet50w2',\n",
        "    'resnet50w4',\n",
        "    'resnet101',\n",
        "    'resnet101w2',\n",
        "    'resnet151',\n",
        "    'resnet151w2',\n",
        "    'resnet200',\n",
        "    'resnet200w2'\n",
        "]\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(\n",
        "        in_planes,\n",
        "        out_planes,\n",
        "        kernel_size=3,\n",
        "        stride=stride,\n",
        "        padding=dilation,\n",
        "        groups=groups,\n",
        "        bias=False,\n",
        "        dilation=dilation,\n",
        "    )\n",
        "\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "    __constants__ = [\"downsample\"]\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        inplanes,\n",
        "        planes,\n",
        "        stride=1,\n",
        "        downsample=None,\n",
        "        groups=1,\n",
        "        base_width=64,\n",
        "        dilation=1,\n",
        "        norm_layer=None,\n",
        "    ):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        if groups != 1 or base_width != 64:\n",
        "            raise ValueError(\"BasicBlock only supports groups=1 and base_width=64\")\n",
        "        if dilation > 1:\n",
        "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
        "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        inplanes,\n",
        "        planes,\n",
        "        stride=1,\n",
        "        downsample=None,\n",
        "        groups=1,\n",
        "        base_width=64,\n",
        "        dilation=1,\n",
        "        norm_layer=None,\n",
        "    ):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        width = int(planes * (base_width / 64.0)) * groups\n",
        "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv1x1(inplanes, width)\n",
        "        self.bn1 = norm_layer(width)\n",
        "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
        "        self.bn2 = norm_layer(width)\n",
        "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
        "        self.bn3 = norm_layer(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        block,\n",
        "        layers,\n",
        "        zero_init_residual=False,\n",
        "        groups=1,\n",
        "        widen=1,\n",
        "        width_per_group=64,\n",
        "        replace_stride_with_dilation=None,\n",
        "        norm_layer=None,\n",
        "        use_maxpool=True,\n",
        "        cifar=False,\n",
        "        num_classes=100,\n",
        "        detach=None\n",
        "    ):\n",
        "        super(ResNet, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        self._norm_layer = norm_layer\n",
        "        self.use_maxpool = use_maxpool\n",
        "        self.detach = detach\n",
        "\n",
        "        self.inplanes = width_per_group * widen\n",
        "        self.dilation = 1\n",
        "        if replace_stride_with_dilation is None:\n",
        "            # each element in the tuple indicates if we should replace\n",
        "            # the 2x2 stride with a dilated convolution instead\n",
        "            replace_stride_with_dilation = [False, False, False]\n",
        "        if len(replace_stride_with_dilation) != 3:\n",
        "            raise ValueError(\n",
        "                \"replace_stride_with_dilation should be None \"\n",
        "                \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation)\n",
        "            )\n",
        "        self.groups = groups\n",
        "        self.base_width = width_per_group\n",
        "\n",
        "        num_out_filters = width_per_group * widen\n",
        "        self.bn1 = norm_layer(num_out_filters)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        if cifar:\n",
        "            self.conv1 = nn.Conv2d(\n",
        "                3, 64, kernel_size=3, stride=1, padding=2, bias=False\n",
        "            )\n",
        "            self.maxpool = nn.Identity()\n",
        "        else:\n",
        "            self.conv1 = nn.Conv2d(\n",
        "                3, num_out_filters, kernel_size=7, stride=2, padding=3, bias=False\n",
        "            )\n",
        "            self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "\n",
        "        self.layer1 = self._make_layer(block, num_out_filters, layers[0])\n",
        "        num_out_filters *= 2\n",
        "        self.layer2 = self._make_layer(\n",
        "            block, num_out_filters, layers[1], stride=2, dilate=replace_stride_with_dilation[0]\n",
        "        )\n",
        "        num_out_filters *= 2\n",
        "        self.layer3 = self._make_layer(\n",
        "            block, num_out_filters, layers[2], stride=2, dilate=replace_stride_with_dilation[1]\n",
        "        )\n",
        "        num_out_filters *= 2\n",
        "        self.layer4 = self._make_layer(\n",
        "            block, num_out_filters, layers[3], stride=2, dilate=replace_stride_with_dilation[2]\n",
        "        )\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        self.fc = None\n",
        "        self.pred = None\n",
        "\n",
        "        self.feat_proj = None\n",
        "        self.feat_gene = None\n",
        "\n",
        "        self.classifier = nn.Linear(num_out_filters, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck):\n",
        "                    nn.init.constant_(m.bn3.weight, 0)\n",
        "                elif isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
        "        norm_layer = self._norm_layer\n",
        "        downsample = None\n",
        "        previous_dilation = self.dilation\n",
        "        if dilate:\n",
        "            self.dilation *= stride\n",
        "            stride = 1\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                norm_layer(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(\n",
        "            block(\n",
        "                self.inplanes,\n",
        "                planes,\n",
        "                stride,\n",
        "                downsample,\n",
        "                self.groups,\n",
        "                self.base_width,\n",
        "                previous_dilation,\n",
        "                norm_layer,\n",
        "            )\n",
        "        )\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(\n",
        "                block(\n",
        "                    self.inplanes,\n",
        "                    planes,\n",
        "                    groups=self.groups,\n",
        "                    base_width=self.base_width,\n",
        "                    dilation=self.dilation,\n",
        "                    norm_layer=norm_layer,\n",
        "                )\n",
        "            )\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _forward_backbone(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        if self.use_maxpool:\n",
        "            x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        if self.detach:\n",
        "            l = self.classifier(x.detach())\n",
        "        else:\n",
        "            l = self.classifier(x)\n",
        "        if self.fc is not None:\n",
        "            x = self.fc(x)\n",
        "        return x, l\n",
        "\n",
        "    def _forward_head(self, x):\n",
        "        if self.pred is not None:\n",
        "            x = self.pred(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, inputs, return_before_head=False):\n",
        "        if not isinstance(inputs, list):\n",
        "            inputs = [inputs]\n",
        "        idx_crops = torch.cumsum(torch.unique_consecutive(\n",
        "            torch.tensor([inp.shape[-1] for inp in inputs]),\n",
        "            return_counts=True,\n",
        "        )[1], 0)\n",
        "        start_idx = 0\n",
        "        for end_idx in idx_crops:\n",
        "            _h, _l = self._forward_backbone(torch.cat(inputs[start_idx:end_idx]))\n",
        "            _z = self._forward_head(_h)\n",
        "            if start_idx == 0:\n",
        "                h, z, l = _h, _z, _l\n",
        "            else:\n",
        "                h, z, l = torch.cat((h, _h)), torch.cat((z, _z)), torch.cat((l, _l))\n",
        "            start_idx = end_idx\n",
        "        if return_before_head:\n",
        "            return h, z, l\n",
        "        return z\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        # TODO: reset parameters\n",
        "        def weight_reset(m, type=None):\n",
        "            if type == 'fc':\n",
        "                m.fc1.reset_parameters()\n",
        "                # m.bn1.reset_parameters()\n",
        "                m.fc2.reset_parameters()\n",
        "                # m.bn2.reset_parameters()\n",
        "                m.fc3.reset_parameters()\n",
        "            elif type == 'conv':\n",
        "                for layer in m:\n",
        "                    layer.conv1.reset_parameters()\n",
        "                    # layer.bn1.reset_parameters()\n",
        "                    layer.conv2.reset_parameters()\n",
        "                    # layer.bn2.reset_parameters()\n",
        "            elif type == 'linear':\n",
        "                m.reset_parameters()\n",
        "\n",
        "        weight_reset(self.fc, type='fc')\n",
        "        # weight_reset(self.layer3, type='conv')\n",
        "        weight_reset(self.layer4, type='conv')\n",
        "        weight_reset(self.classifier, type='linear')\n",
        "\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def partial_reset_parameters(self, reset_prob=0.5):\n",
        "        assert 0<=reset_prob<=1\n",
        "\n",
        "        def apply_reset(n, bn=False):\n",
        "            with torch.no_grad():\n",
        "                # TODO : To see if batch norm needs to be partial\n",
        "                if bn:\n",
        "                    n.reset_parameters()\n",
        "                else:\n",
        "                    device = n.weight.device\n",
        "                    mask = (torch.rand(n.weight.shape) < reset_prob).type(torch.LongTensor).to(device)\n",
        "                    val = n.weight.detach().clone()\n",
        "                    init.kaiming_uniform_(val, a=math.sqrt(5))\n",
        "                    # print('mask shape: {}, non zero: {}'.format(mask.shape, mask.sum()))\n",
        "                    # val = 0.0 # if not bn else 1.0\n",
        "                    # tensor.weight.data is not efficient because it creates new tensor\n",
        "                    # n.weight.data[mask].fill_(val)\n",
        "                    n.weight.mul_((1-mask)).add_(mask*val)\n",
        "\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        def weight_reset(m, type=None):\n",
        "            if type == 'fc':\n",
        "                apply_reset(m.fc1)\n",
        "                apply_reset(m.bn1, bn=True)\n",
        "                apply_reset(m.fc2)\n",
        "                apply_reset(m.bn2, bn=True)\n",
        "                apply_reset(m.fc3)\n",
        "            elif type == 'conv':\n",
        "                for layer in m:\n",
        "                    apply_reset(layer.conv1)\n",
        "                    apply_reset(layer.bn1, bn=True)\n",
        "                    apply_reset(layer.conv2)\n",
        "                    apply_reset(layer.bn2, bn=True)\n",
        "            elif type == 'linear':\n",
        "                apply_reset(m)\n",
        "\n",
        "        weight_reset(self.fc, type='fc')\n",
        "        # weight_reset(self.layer3, type='conv')\n",
        "        weight_reset(self.layer4, type='conv')\n",
        "        # weight_reset(self.classifier, type='linear')\n",
        "\n",
        "    def reset_proj(self):\n",
        "        def weight_reset(m, type=None):\n",
        "            if type == 'fc':\n",
        "                m.fc1.reset_parameters()\n",
        "                m.bn1.reset_parameters()\n",
        "                m.fc2.reset_parameters()\n",
        "                # m.bn2.reset_parameters()\n",
        "                m.fc3.reset_parameters()\n",
        "\n",
        "        if self.feat_proj is not None:\n",
        "            weight_reset(self.fc, type='fc')\n",
        "\n",
        "def resnet18(**kwargs):\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
        "\n",
        "\n",
        "def resnet50(**kwargs):\n",
        "    return ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
        "\n",
        "\n",
        "def resnet50w2(**kwargs):\n",
        "    return ResNet(Bottleneck, [3, 4, 6, 3], widen=2, **kwargs)\n",
        "\n",
        "\n",
        "def resnet50w4(**kwargs):\n",
        "    return ResNet(Bottleneck, [3, 4, 6, 3], widen=4, **kwargs)\n",
        "\n",
        "\n",
        "def resnet101(**kwargs):\n",
        "    return ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n",
        "\n",
        "\n",
        "def resnet101w2(**kwargs):\n",
        "    return ResNet(Bottleneck, [3, 4, 23, 3], widen=2, **kwargs)\n",
        "\n",
        "\n",
        "def resnet151(**kwargs):\n",
        "    return ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n",
        "\n",
        "\n",
        "def resnet151w2(**kwargs):\n",
        "    return ResNet(Bottleneck, [3, 8, 36, 3], widen=2, **kwargs)\n",
        "\n",
        "\n",
        "def resnet200(**kwargs):\n",
        "    return ResNet(Bottleneck, [3, 24, 36, 3], **kwargs)\n",
        "\n",
        "\n",
        "def resnet200w2(**kwargs):\n",
        "    return ResNet(Bottleneck, [3, 24, 36, 3], widen=2, **kwargs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vg9ltzgsyNO9"
      },
      "outputs": [],
      "source": [
        "import resnet as resnet\n",
        "def init_model(\n",
        "    device,\n",
        "    model_name='resnet50',\n",
        "    use_pred=False,\n",
        "    output_dim=128,\n",
        "    cifar=False,\n",
        "    num_classes=100,\n",
        "    detach=None\n",
        "):\n",
        "    assert detach is not None\n",
        "    if 'wide_resnet' in model_name:\n",
        "        encoder = wide_resnet.__dict__[model_name](dropout_rate=0.0)\n",
        "        hidden_dim = 128\n",
        "    else:\n",
        "        encoder = resnet.__dict__[model_name](cifar=cifar, num_classes=num_classes, detach=detach)\n",
        "        if model_name == 'resnet18':\n",
        "            hidden_dim = 512\n",
        "        else:\n",
        "            hidden_dim = 2048\n",
        "            if 'w2' in model_name:\n",
        "                hidden_dim *= 2\n",
        "            elif 'w4' in model_name:\n",
        "                hidden_dim *= 4\n",
        "\n",
        "    # -- projection head\n",
        "    encoder.fc = torch.nn.Sequential(OrderedDict([\n",
        "        ('fc1', torch.nn.Linear(hidden_dim, hidden_dim)),\n",
        "        ('bn1', torch.nn.BatchNorm1d(hidden_dim)),\n",
        "        ('relu1', torch.nn.ReLU(inplace=True)),\n",
        "        ('fc2', torch.nn.Linear(hidden_dim, hidden_dim)),\n",
        "        ('bn2', torch.nn.BatchNorm1d(hidden_dim)),\n",
        "        ('relu2', torch.nn.ReLU(inplace=True)),\n",
        "        ('fc3', torch.nn.Linear(hidden_dim, output_dim))\n",
        "    ]))\n",
        "\n",
        "    # -- projection head for feature alignment\n",
        "    encoder.feat_proj = torch.nn.Sequential(OrderedDict([\n",
        "        ('fc1', torch.nn.Linear(output_dim, output_dim)),\n",
        "    ]))\n",
        "\n",
        "\n",
        "    # -- prediction head\n",
        "    encoder.pred = None\n",
        "    if use_pred:\n",
        "        mx = 4  # 4x bottleneck prediction head\n",
        "        pred_head = OrderedDict([])\n",
        "        pred_head['bn1'] = torch.nn.BatchNorm1d(output_dim)\n",
        "        pred_head['fc1'] = torch.nn.Linear(output_dim, output_dim//mx)\n",
        "        pred_head['bn2'] = torch.nn.BatchNorm1d(output_dim//mx)\n",
        "        pred_head['relu'] = torch.nn.ReLU(inplace=True)\n",
        "        pred_head['fc2'] = torch.nn.Linear(output_dim//mx, output_dim)\n",
        "        encoder.pred = torch.nn.Sequential(pred_head)\n",
        "\n",
        "    encoder.to(device)\n",
        "    logger.info(encoder)\n",
        "    return encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "POFRkRqczwfe",
        "outputId": "cca67669-035f-4fb3-c17e-c7d2ca87945a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtVs_Wtsz75E"
      },
      "outputs": [],
      "source": [
        "model_name = \"resnet18\"\n",
        "use_pred_head = False\n",
        "output_dim = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LD56QOeNzUQi"
      },
      "outputs": [],
      "source": [
        "encoder = init_model(\n",
        "    device=device,\n",
        "    model_name=model_name,\n",
        "    use_pred=use_pred_head,\n",
        "    output_dim=output_dim,\n",
        "    cifar= 'cifar10',\n",
        "    num_classes=10,\n",
        "    detach=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "MYN8FKnO029o"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U torchinfo\n",
        "from torchinfo import summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-9-0G_f09Zp",
        "outputId": "eac8903b-632e-44ee-f6f5-6f3d2fef8d27"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "========================================================================================================================\n",
              "Layer (type (var_name))                  Input Shape          Output Shape         Param #              Trainable\n",
              "========================================================================================================================\n",
              "ResNet (ResNet)                          [32, 3, 32, 32]      [32, 128]            16,512               True\n",
              "├─Conv2d (conv1)                         [32, 3, 32, 32]      [32, 64, 34, 34]     1,728                True\n",
              "├─BatchNorm2d (bn1)                      [32, 64, 34, 34]     [32, 64, 34, 34]     128                  True\n",
              "├─ReLU (relu)                            [32, 64, 34, 34]     [32, 64, 34, 34]     --                   --\n",
              "├─Identity (maxpool)                     [32, 64, 34, 34]     [32, 64, 34, 34]     --                   --\n",
              "├─Sequential (layer1)                    [32, 64, 34, 34]     [32, 64, 34, 34]     --                   True\n",
              "│    └─BasicBlock (0)                    [32, 64, 34, 34]     [32, 64, 34, 34]     --                   True\n",
              "│    │    └─Conv2d (conv1)               [32, 64, 34, 34]     [32, 64, 34, 34]     36,864               True\n",
              "│    │    └─BatchNorm2d (bn1)            [32, 64, 34, 34]     [32, 64, 34, 34]     128                  True\n",
              "│    │    └─ReLU (relu)                  [32, 64, 34, 34]     [32, 64, 34, 34]     --                   --\n",
              "│    │    └─Conv2d (conv2)               [32, 64, 34, 34]     [32, 64, 34, 34]     36,864               True\n",
              "│    │    └─BatchNorm2d (bn2)            [32, 64, 34, 34]     [32, 64, 34, 34]     128                  True\n",
              "│    │    └─ReLU (relu)                  [32, 64, 34, 34]     [32, 64, 34, 34]     --                   --\n",
              "│    └─BasicBlock (1)                    [32, 64, 34, 34]     [32, 64, 34, 34]     --                   True\n",
              "│    │    └─Conv2d (conv1)               [32, 64, 34, 34]     [32, 64, 34, 34]     36,864               True\n",
              "│    │    └─BatchNorm2d (bn1)            [32, 64, 34, 34]     [32, 64, 34, 34]     128                  True\n",
              "│    │    └─ReLU (relu)                  [32, 64, 34, 34]     [32, 64, 34, 34]     --                   --\n",
              "│    │    └─Conv2d (conv2)               [32, 64, 34, 34]     [32, 64, 34, 34]     36,864               True\n",
              "│    │    └─BatchNorm2d (bn2)            [32, 64, 34, 34]     [32, 64, 34, 34]     128                  True\n",
              "│    │    └─ReLU (relu)                  [32, 64, 34, 34]     [32, 64, 34, 34]     --                   --\n",
              "├─Sequential (layer2)                    [32, 64, 34, 34]     [32, 128, 17, 17]    --                   True\n",
              "│    └─BasicBlock (0)                    [32, 64, 34, 34]     [32, 128, 17, 17]    --                   True\n",
              "│    │    └─Conv2d (conv1)               [32, 64, 34, 34]     [32, 128, 17, 17]    73,728               True\n",
              "│    │    └─BatchNorm2d (bn1)            [32, 128, 17, 17]    [32, 128, 17, 17]    256                  True\n",
              "│    │    └─ReLU (relu)                  [32, 128, 17, 17]    [32, 128, 17, 17]    --                   --\n",
              "│    │    └─Conv2d (conv2)               [32, 128, 17, 17]    [32, 128, 17, 17]    147,456              True\n",
              "│    │    └─BatchNorm2d (bn2)            [32, 128, 17, 17]    [32, 128, 17, 17]    256                  True\n",
              "│    │    └─Sequential (downsample)      [32, 64, 34, 34]     [32, 128, 17, 17]    8,448                True\n",
              "│    │    └─ReLU (relu)                  [32, 128, 17, 17]    [32, 128, 17, 17]    --                   --\n",
              "│    └─BasicBlock (1)                    [32, 128, 17, 17]    [32, 128, 17, 17]    --                   True\n",
              "│    │    └─Conv2d (conv1)               [32, 128, 17, 17]    [32, 128, 17, 17]    147,456              True\n",
              "│    │    └─BatchNorm2d (bn1)            [32, 128, 17, 17]    [32, 128, 17, 17]    256                  True\n",
              "│    │    └─ReLU (relu)                  [32, 128, 17, 17]    [32, 128, 17, 17]    --                   --\n",
              "│    │    └─Conv2d (conv2)               [32, 128, 17, 17]    [32, 128, 17, 17]    147,456              True\n",
              "│    │    └─BatchNorm2d (bn2)            [32, 128, 17, 17]    [32, 128, 17, 17]    256                  True\n",
              "│    │    └─ReLU (relu)                  [32, 128, 17, 17]    [32, 128, 17, 17]    --                   --\n",
              "├─Sequential (layer3)                    [32, 128, 17, 17]    [32, 256, 9, 9]      --                   True\n",
              "│    └─BasicBlock (0)                    [32, 128, 17, 17]    [32, 256, 9, 9]      --                   True\n",
              "│    │    └─Conv2d (conv1)               [32, 128, 17, 17]    [32, 256, 9, 9]      294,912              True\n",
              "│    │    └─BatchNorm2d (bn1)            [32, 256, 9, 9]      [32, 256, 9, 9]      512                  True\n",
              "│    │    └─ReLU (relu)                  [32, 256, 9, 9]      [32, 256, 9, 9]      --                   --\n",
              "│    │    └─Conv2d (conv2)               [32, 256, 9, 9]      [32, 256, 9, 9]      589,824              True\n",
              "│    │    └─BatchNorm2d (bn2)            [32, 256, 9, 9]      [32, 256, 9, 9]      512                  True\n",
              "│    │    └─Sequential (downsample)      [32, 128, 17, 17]    [32, 256, 9, 9]      33,280               True\n",
              "│    │    └─ReLU (relu)                  [32, 256, 9, 9]      [32, 256, 9, 9]      --                   --\n",
              "│    └─BasicBlock (1)                    [32, 256, 9, 9]      [32, 256, 9, 9]      --                   True\n",
              "│    │    └─Conv2d (conv1)               [32, 256, 9, 9]      [32, 256, 9, 9]      589,824              True\n",
              "│    │    └─BatchNorm2d (bn1)            [32, 256, 9, 9]      [32, 256, 9, 9]      512                  True\n",
              "│    │    └─ReLU (relu)                  [32, 256, 9, 9]      [32, 256, 9, 9]      --                   --\n",
              "│    │    └─Conv2d (conv2)               [32, 256, 9, 9]      [32, 256, 9, 9]      589,824              True\n",
              "│    │    └─BatchNorm2d (bn2)            [32, 256, 9, 9]      [32, 256, 9, 9]      512                  True\n",
              "│    │    └─ReLU (relu)                  [32, 256, 9, 9]      [32, 256, 9, 9]      --                   --\n",
              "├─Sequential (layer4)                    [32, 256, 9, 9]      [32, 512, 5, 5]      --                   True\n",
              "│    └─BasicBlock (0)                    [32, 256, 9, 9]      [32, 512, 5, 5]      --                   True\n",
              "│    │    └─Conv2d (conv1)               [32, 256, 9, 9]      [32, 512, 5, 5]      1,179,648            True\n",
              "│    │    └─BatchNorm2d (bn1)            [32, 512, 5, 5]      [32, 512, 5, 5]      1,024                True\n",
              "│    │    └─ReLU (relu)                  [32, 512, 5, 5]      [32, 512, 5, 5]      --                   --\n",
              "│    │    └─Conv2d (conv2)               [32, 512, 5, 5]      [32, 512, 5, 5]      2,359,296            True\n",
              "│    │    └─BatchNorm2d (bn2)            [32, 512, 5, 5]      [32, 512, 5, 5]      1,024                True\n",
              "│    │    └─Sequential (downsample)      [32, 256, 9, 9]      [32, 512, 5, 5]      132,096              True\n",
              "│    │    └─ReLU (relu)                  [32, 512, 5, 5]      [32, 512, 5, 5]      --                   --\n",
              "│    └─BasicBlock (1)                    [32, 512, 5, 5]      [32, 512, 5, 5]      --                   True\n",
              "│    │    └─Conv2d (conv1)               [32, 512, 5, 5]      [32, 512, 5, 5]      2,359,296            True\n",
              "│    │    └─BatchNorm2d (bn1)            [32, 512, 5, 5]      [32, 512, 5, 5]      1,024                True\n",
              "│    │    └─ReLU (relu)                  [32, 512, 5, 5]      [32, 512, 5, 5]      --                   --\n",
              "│    │    └─Conv2d (conv2)               [32, 512, 5, 5]      [32, 512, 5, 5]      2,359,296            True\n",
              "│    │    └─BatchNorm2d (bn2)            [32, 512, 5, 5]      [32, 512, 5, 5]      1,024                True\n",
              "│    │    └─ReLU (relu)                  [32, 512, 5, 5]      [32, 512, 5, 5]      --                   --\n",
              "├─AdaptiveAvgPool2d (avgpool)            [32, 512, 5, 5]      [32, 512, 1, 1]      --                   --\n",
              "├─Linear (classifier)                    [32, 512]            [32, 10]             5,130                True\n",
              "├─Sequential (fc)                        [32, 512]            [32, 128]            --                   True\n",
              "│    └─Linear (fc1)                      [32, 512]            [32, 512]            262,656              True\n",
              "│    └─BatchNorm1d (bn1)                 [32, 512]            [32, 512]            1,024                True\n",
              "│    └─ReLU (relu1)                      [32, 512]            [32, 512]            --                   --\n",
              "│    └─Linear (fc2)                      [32, 512]            [32, 512]            262,656              True\n",
              "│    └─BatchNorm1d (bn2)                 [32, 512]            [32, 512]            1,024                True\n",
              "│    └─ReLU (relu2)                      [32, 512]            [32, 512]            --                   --\n",
              "│    └─Linear (fc3)                      [32, 512]            [32, 128]            65,664               True\n",
              "========================================================================================================================\n",
              "Total params: 11,783,498\n",
              "Trainable params: 11,783,498\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (G): 22.53\n",
              "========================================================================================================================\n",
              "Input size (MB): 0.39\n",
              "Forward/backward pass size (MB): 370.51\n",
              "Params size (MB): 47.07\n",
              "Estimated Total Size (MB): 417.97\n",
              "========================================================================================================================"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "summary(model=encoder,\n",
        "        input_size=(32, 3, 32, 32),\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Copyright (c) Facebook, Inc. and its affiliates.\n",
        "# All rights reserved.\n",
        "#\n",
        "# This source code is licensed under the license found in the\n",
        "# LICENSE file in the root directory of this source tree.\n",
        "#\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "from logging import getLogger\n",
        "\n",
        "import numpy as np\n",
        "from math import ceil\n",
        "import random \n",
        "\n",
        "import torch\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision\n",
        "\n",
        "import PIL\n",
        "from PIL import Image\n",
        "from PIL import ImageFilter\n",
        "from PIL import ImageOps\n",
        "\n",
        "_GLOBAL_SEED = 0\n",
        "logger = getLogger()\n",
        "\n",
        "def seed_worker(worker_id):\n",
        "    worker_seed = torch.initial_seed() % 2**32\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n",
        "\n",
        "g = torch.Generator()\n",
        "g.manual_seed(0)\n",
        "\n",
        "def init_data(\n",
        "    dataset_name,\n",
        "    transform,\n",
        "    init_transform,\n",
        "    u_batch_size,\n",
        "    s_batch_size,\n",
        "    classes_per_batch,\n",
        "    unique_classes=False,\n",
        "    multicrop_transform=(0, None),\n",
        "    supervised_views=1,\n",
        "    world_size=1,\n",
        "    rank=0,\n",
        "    root_path=None,\n",
        "    image_folder=None,\n",
        "    training=True,\n",
        "    copy_data=False,\n",
        "    stratify=False,\n",
        "    drop_last=True,\n",
        "    tasks=None,\n",
        "    task_idx=0,\n",
        "    visible_class_ul=None,\n",
        "    us=False,\n",
        "    buffer_lst=None\n",
        "):\n",
        "    \"\"\"\n",
        "    :param dataset_name: ['imagenet', 'cifar10', 'cifar10_fine_tune', 'imagenet_fine_tune']\n",
        "    :param transform: torchvision transform to apply to each batch of data\n",
        "    :param init_transform: transform to apply once to all data at the start\n",
        "    :param u_batch_size: unsupervised batch-size\n",
        "    :param s_batch_size: supervised batch-size (images per class)\n",
        "    :param classes_per_batch: num. classes sampled in each supervised batch per gpu\n",
        "    :param unique_classes: whether each GPU should load different classes\n",
        "    :param multicrop_transform: number of smaller multi-crop images to return\n",
        "    :param supervised_views: number of views to generate of each labeled imgs\n",
        "    :param world_size: number of workers for distributed training\n",
        "    :param rank: rank of worker in distributed training\n",
        "    :param root_path: path to the root directory containing all dataset\n",
        "    :param image_folder: name of folder in 'root_path' containing data to load\n",
        "    :param training: whether to load training data\n",
        "    :param stratify: whether to class stratify 'fine_tune' data loaders\n",
        "    :param copy_data: whether to copy data locally to node at start of training\n",
        "    \"\"\"\n",
        "\n",
        "    if dataset_name == 'imagenet':\n",
        "        return _init_imgnt_data(\n",
        "            transform=transform,\n",
        "            init_transform=init_transform,\n",
        "            u_batch_size=u_batch_size,\n",
        "            s_batch_size=s_batch_size,\n",
        "            classes_per_batch=classes_per_batch,\n",
        "            unique_classes=unique_classes,\n",
        "            multicrop_transform=multicrop_transform,\n",
        "            supervised_views=supervised_views,\n",
        "            world_size=world_size,\n",
        "            rank=rank,\n",
        "            root_path=root_path,\n",
        "            image_folder=image_folder,\n",
        "            training=training,\n",
        "            copy_data=copy_data,\n",
        "            tasks=tasks,\n",
        "            task_idx=task_idx,\n",
        "            visible_class_ul=visible_class_ul,\n",
        "            buffer_lst=buffer_lst)\n",
        "\n",
        "    elif dataset_name == 'imagenet_fine_tune':\n",
        "        batch_size = s_batch_size\n",
        "        return _init_imgnt_ft_data(\n",
        "            transform=transform,\n",
        "            init_transform=init_transform,\n",
        "            batch_size=batch_size,\n",
        "            stratify=stratify,\n",
        "            classes_per_batch=classes_per_batch,\n",
        "            unique_classes=unique_classes,\n",
        "            world_size=world_size,\n",
        "            rank=rank,\n",
        "            root_path=root_path,\n",
        "            image_folder=image_folder,\n",
        "            training=training,\n",
        "            drop_last=drop_last,\n",
        "            copy_data=copy_data,\n",
        "            tasks=tasks,\n",
        "            task_idx=task_idx,\n",
        "            visible_class_ul=visible_class_ul)\n",
        "\n",
        "\n",
        "    elif dataset_name == 'cifar10':\n",
        "        return _init_cifar10_data(\n",
        "            transform=transform,\n",
        "            init_transform=init_transform,\n",
        "            u_batch_size=u_batch_size,\n",
        "            s_batch_size=s_batch_size,\n",
        "            classes_per_batch=classes_per_batch,\n",
        "            multicrop_transform=multicrop_transform,\n",
        "            supervised_views=supervised_views,\n",
        "            world_size=world_size,\n",
        "            rank=rank,\n",
        "            root_path=root_path,\n",
        "            image_folder=image_folder,\n",
        "            training=training,\n",
        "            copy_data=copy_data,\n",
        "            tasks=tasks,\n",
        "            task_idx=task_idx,\n",
        "            visible_class_ul=visible_class_ul,\n",
        "            us=us,\n",
        "            buffer_lst=buffer_lst)\n",
        "\n",
        "    elif dataset_name == 'cifar10_fine_tune':\n",
        "        batch_size = s_batch_size\n",
        "        return _init_cifar10_ft_data(\n",
        "            transform=transform,\n",
        "            init_transform=init_transform,\n",
        "            supervised_views=supervised_views,\n",
        "            batch_size=batch_size,\n",
        "            stratify=stratify,\n",
        "            classes_per_batch=classes_per_batch,\n",
        "            unique_classes=unique_classes,\n",
        "            world_size=world_size,\n",
        "            rank=rank,\n",
        "            drop_last=drop_last,\n",
        "            root_path=root_path,\n",
        "            image_folder=image_folder,\n",
        "            training=training,\n",
        "            copy_data=copy_data,\n",
        "            tasks=tasks,\n",
        "            task_idx=task_idx,\n",
        "            visible_class_ul=visible_class_ul)\n",
        "\n",
        "    elif dataset_name == 'cifar100':\n",
        "        return _init_cifar100_data(\n",
        "            transform=transform,\n",
        "            init_transform=init_transform,\n",
        "            u_batch_size=u_batch_size,\n",
        "            s_batch_size=s_batch_size,\n",
        "            classes_per_batch=classes_per_batch,\n",
        "            multicrop_transform=multicrop_transform,\n",
        "            supervised_views=supervised_views,\n",
        "            world_size=world_size,\n",
        "            rank=rank,\n",
        "            root_path=root_path,\n",
        "            image_folder=image_folder,\n",
        "            training=training,\n",
        "            copy_data=copy_data,\n",
        "            tasks=tasks,\n",
        "            task_idx=task_idx,\n",
        "            visible_class_ul=visible_class_ul,\n",
        "            buffer_lst=buffer_lst)\n",
        "\n",
        "    elif dataset_name == 'cifar100_fine_tune':\n",
        "        batch_size = s_batch_size\n",
        "        return _init_cifar100_ft_data(\n",
        "            transform=transform,\n",
        "            init_transform=init_transform,\n",
        "            supervised_views=supervised_views,\n",
        "            batch_size=batch_size,\n",
        "            stratify=stratify,\n",
        "            classes_per_batch=classes_per_batch,\n",
        "            unique_classes=unique_classes,\n",
        "            world_size=world_size,\n",
        "            rank=rank,\n",
        "            drop_last=drop_last,\n",
        "            root_path=root_path,\n",
        "            image_folder=image_folder,\n",
        "            training=training,\n",
        "            copy_data=copy_data,\n",
        "            tasks=tasks,\n",
        "            task_idx=task_idx,\n",
        "            visible_class_ul=visible_class_ul)\n",
        "\n",
        "\n",
        "def _init_cifar10_ft_data(\n",
        "    transform,\n",
        "    init_transform,\n",
        "    batch_size,\n",
        "    stratify=False,\n",
        "    classes_per_batch=1,\n",
        "    unique_classes=False,\n",
        "    supervised_views=1,\n",
        "    world_size=1,\n",
        "    rank=0,\n",
        "    root_path='/datasets/',\n",
        "    image_folder='cifar-pytorch/11222017/',\n",
        "    training=True,\n",
        "    copy_data=False,\n",
        "    drop_last=False,\n",
        "    tasks=None,\n",
        "    task_idx=0,\n",
        "    visible_class_ul=None\n",
        "):\n",
        "    dataset = TransCIFAR10(\n",
        "        root=root_path,\n",
        "        image_folder=image_folder,\n",
        "        copy_data=copy_data,\n",
        "        transform=transform,\n",
        "        init_transform=init_transform,\n",
        "        supervised_views=supervised_views,\n",
        "        train=training,\n",
        "        supervised=True,\n",
        "        tasks=sum(tasks[:task_idx + 1], []),\n",
        "        task_idx=task_idx)\n",
        "\n",
        "    if not stratify:\n",
        "        dist_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "            dataset=dataset,\n",
        "            num_replicas=world_size,\n",
        "            rank=rank)\n",
        "        data_loader = torch.utils.data.DataLoader(\n",
        "            dataset,\n",
        "            sampler=dist_sampler,\n",
        "            batch_size=batch_size,\n",
        "            drop_last=drop_last,\n",
        "            pin_memory=True,\n",
        "            num_workers=8,\n",
        "            worker_init_fn=seed_worker,\n",
        "            generator=g)\n",
        "    else:\n",
        "        dist_sampler = ClassStratifiedSampler(\n",
        "            data_source=dataset,\n",
        "            world_size=world_size,\n",
        "            rank=rank,\n",
        "            batch_size=batch_size,\n",
        "            classes_per_batch=classes_per_batch,\n",
        "            seed=_GLOBAL_SEED,\n",
        "            unique_classes=unique_classes)\n",
        "        data_loader = torch.utils.data.DataLoader(\n",
        "            dataset,\n",
        "            batch_sampler=dist_sampler,\n",
        "            pin_memory=True,\n",
        "            num_workers=8,\n",
        "            worker_init_fn=seed_worker,\n",
        "            generator=g)\n",
        "\n",
        "    return (data_loader, dist_sampler)\n",
        "\n",
        "\n",
        "def _init_cifar10_data(\n",
        "    transform,\n",
        "    init_transform,\n",
        "    u_batch_size,\n",
        "    s_batch_size,\n",
        "    classes_per_batch=10,\n",
        "    supervised_transform=None,\n",
        "    multicrop_transform=(0, None),\n",
        "    supervised_views=1,\n",
        "    world_size=1,\n",
        "    rank=0,\n",
        "    root_path='/datasets/',\n",
        "    image_folder='cifar-pytorch/11222017/',\n",
        "    training=True,\n",
        "    copy_data=False,\n",
        "    tasks=None,\n",
        "    task_idx=0,\n",
        "    visible_class_ul=None,\n",
        "    us=False,\n",
        "    buffer_lst=None\n",
        "):\n",
        "    unsupervised_set = TransCIFAR10(\n",
        "        root=root_path,\n",
        "        image_folder=image_folder,\n",
        "        copy_data=copy_data,\n",
        "        transform=transform,\n",
        "        init_transform=init_transform,\n",
        "        multicrop_transform=multicrop_transform,\n",
        "        train=training,\n",
        "        supervised=False)\n",
        "    if tasks is not None:\n",
        "        if visible_class_ul is None:\n",
        "            visible_class_ul = tasks[task_idx]\n",
        "        else:\n",
        "            if len(visible_class_ul) != len(tasks[task_idx]): # That means we are not in current mode\n",
        "                print('!!!!!!!!Not in current mode!')\n",
        "                pre_classes = sum(tasks[:task_idx], [])\n",
        "\n",
        "        indexes = [i for i, c in enumerate(unsupervised_set.targets) if c in visible_class_ul]\n",
        "\n",
        "\n",
        "        num_unlab = int(len(indexes) / len(visible_class_ul))\n",
        "        indexes = random.sample(indexes, k=num_unlab)\n",
        "\n",
        "        unsupervised_set = torch.utils.data.Subset(unsupervised_set, indexes)\n",
        "\n",
        "    unsupervised_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "        dataset=unsupervised_set,\n",
        "        num_replicas=world_size,\n",
        "        rank=rank)\n",
        "    unsupervised_loader = torch.utils.data.DataLoader(\n",
        "        unsupervised_set,\n",
        "        sampler=unsupervised_sampler,\n",
        "        batch_size=u_batch_size,\n",
        "        drop_last=True,\n",
        "        pin_memory=True,\n",
        "        num_workers=8,\n",
        "        worker_init_fn=seed_worker,\n",
        "        generator=g)\n",
        "\n",
        "    supervised_sampler, supervised_loader = None, None\n",
        "    if classes_per_batch > 0 and s_batch_size > 0:\n",
        "        if us:\n",
        "            supervised_set = TransCIFAR10(\n",
        "                root=root_path,\n",
        "                image_folder=image_folder,\n",
        "                copy_data=copy_data,\n",
        "                transform=supervised_transform if supervised_transform is not None else transform,\n",
        "                supervised_views=supervised_views,\n",
        "                init_transform=init_transform,\n",
        "                multicrop_transform=multicrop_transform,\n",
        "                train=True,\n",
        "                supervised=True,\n",
        "                tasks=sum(tasks[:task_idx + 1], []),\n",
        "                task_idx=task_idx,\n",
        "                us=us)\n",
        "        else:\n",
        "            supervised_set = TransCIFAR10(\n",
        "                root=root_path,\n",
        "                image_folder=image_folder,\n",
        "                copy_data=copy_data,\n",
        "                transform=supervised_transform if supervised_transform is not None else transform,\n",
        "                supervised_views=supervised_views,\n",
        "                init_transform=init_transform,\n",
        "                train=True,\n",
        "                supervised=True,\n",
        "                tasks=sum(tasks[:task_idx + 1], []),\n",
        "                task_idx=task_idx,\n",
        "                us=us,\n",
        "                buffer_lst=buffer_lst)\n",
        "        supervised_sampler = ClassStratifiedSampler(\n",
        "            data_source=supervised_set,\n",
        "            world_size=world_size,\n",
        "            rank=rank,\n",
        "            batch_size=s_batch_size,\n",
        "            classes_per_batch=classes_per_batch,\n",
        "            seed=_GLOBAL_SEED)\n",
        "        supervised_loader = torch.utils.data.DataLoader(\n",
        "            supervised_set,\n",
        "            batch_sampler=supervised_sampler,\n",
        "            num_workers=8,\n",
        "            worker_init_fn=seed_worker,\n",
        "            generator=g)\n",
        "        if len(supervised_loader) > 0:\n",
        "            tmp = ceil(len(unsupervised_loader) / len(supervised_loader))\n",
        "            supervised_sampler.set_inner_epochs(tmp)\n",
        "            logger.debug(f'supervised-reset-period {tmp}')\n",
        "\n",
        "    return (unsupervised_loader, unsupervised_sampler,\n",
        "            supervised_loader, supervised_sampler)\n",
        "\n",
        "\n",
        "def _init_cifar100_ft_data(\n",
        "    transform,\n",
        "    init_transform,\n",
        "    batch_size,\n",
        "    stratify=False,\n",
        "    classes_per_batch=1,\n",
        "    unique_classes=False,\n",
        "    supervised_views=1,\n",
        "    world_size=1,\n",
        "    rank=0,\n",
        "    root_path='/datasets/',\n",
        "    image_folder='cifar-pytorch/11222017/',\n",
        "    training=True,\n",
        "    copy_data=False,\n",
        "    drop_last=False,\n",
        "    tasks=None,\n",
        "    task_idx=0,\n",
        "    visible_class_ul=None\n",
        "):\n",
        "\n",
        "    dataset = TransCIFAR100(\n",
        "        root=root_path,\n",
        "        image_folder=image_folder,\n",
        "        copy_data=copy_data,\n",
        "        transform=transform,\n",
        "        init_transform=init_transform,\n",
        "        supervised_views=supervised_views,\n",
        "        train=training,\n",
        "        supervised=True,\n",
        "        task_idx=task_idx,\n",
        "        tasks=sum(tasks[:task_idx + 1], []))\n",
        "\n",
        "    if not stratify:\n",
        "        dist_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "            dataset=dataset,\n",
        "            num_replicas=world_size,\n",
        "            rank=rank)\n",
        "        data_loader = torch.utils.data.DataLoader(\n",
        "            dataset,\n",
        "            sampler=dist_sampler,\n",
        "            batch_size=batch_size,\n",
        "            drop_last=drop_last,\n",
        "            pin_memory=True,\n",
        "            num_workers=8,\n",
        "            worker_init_fn=seed_worker,\n",
        "            generator=g)\n",
        "    else:\n",
        "        dist_sampler = ClassStratifiedSampler(\n",
        "            data_source=dataset,\n",
        "            world_size=world_size,\n",
        "            rank=rank,\n",
        "            batch_size=batch_size,\n",
        "            classes_per_batch=classes_per_batch,\n",
        "            seed=_GLOBAL_SEED,\n",
        "            unique_classes=unique_classes)\n",
        "        data_loader = torch.utils.data.DataLoader(\n",
        "            dataset,\n",
        "            batch_sampler=dist_sampler,\n",
        "            pin_memory=True,\n",
        "            num_workers=8,\n",
        "            worker_init_fn=seed_worker,\n",
        "            generator=g)\n",
        "\n",
        "    return (data_loader, dist_sampler)\n",
        "\n",
        "\n",
        "def _init_cifar100_data(\n",
        "    transform,\n",
        "    init_transform,\n",
        "    u_batch_size,\n",
        "    s_batch_size,\n",
        "    classes_per_batch=100,\n",
        "    supervised_transform=None,\n",
        "    multicrop_transform=(0, None),\n",
        "    supervised_views=1,\n",
        "    world_size=1,\n",
        "    rank=0,\n",
        "    root_path='/datasets/',\n",
        "    image_folder='cifar-pytorch/11222017/',\n",
        "    training=True,\n",
        "    copy_data=False,\n",
        "    tasks=None,\n",
        "    task_idx=0,\n",
        "    visible_class_ul=None,\n",
        "    buffer_lst=None\n",
        "):\n",
        "    unsupervised_set = TransCIFAR100(\n",
        "        root=root_path,\n",
        "        image_folder=image_folder,\n",
        "        copy_data=copy_data,\n",
        "        transform=transform,\n",
        "        init_transform=init_transform,\n",
        "        multicrop_transform=multicrop_transform,\n",
        "        train=training,\n",
        "        supervised=False)\n",
        "    if tasks is not None:\n",
        "        if visible_class_ul is None:\n",
        "            visible_class_ul = tasks[task_idx]\n",
        "        indexes = [i for i, c in enumerate(unsupervised_set.targets) if c in visible_class_ul]\n",
        "        # Sample a subset of unlabeled data for the case of current_seen\n",
        "        num_unlab = int(len(indexes)) #/ len(visible_class_ul))\n",
        "        indexes = random.sample(indexes, k=num_unlab)\n",
        "        unsupervised_set = torch.utils.data.Subset(unsupervised_set, indexes)\n",
        "    unsupervised_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "        dataset=unsupervised_set,\n",
        "        num_replicas=world_size,\n",
        "        rank=rank)\n",
        "    unsupervised_loader = torch.utils.data.DataLoader(\n",
        "        unsupervised_set,\n",
        "        sampler=unsupervised_sampler,\n",
        "        batch_size=u_batch_size,\n",
        "        drop_last=True,\n",
        "        pin_memory=True,\n",
        "        num_workers=8,\n",
        "        worker_init_fn=seed_worker,\n",
        "        generator=g)\n",
        "\n",
        "    supervised_sampler, supervised_loader = None, None\n",
        "    if classes_per_batch > 0 and s_batch_size > 0:\n",
        "        supervised_set = TransCIFAR100(\n",
        "            root=root_path,\n",
        "            image_folder=image_folder,\n",
        "            copy_data=copy_data,\n",
        "            transform=supervised_transform if supervised_transform is not None else transform,\n",
        "            supervised_views=supervised_views,\n",
        "            init_transform=init_transform,\n",
        "            train=True,\n",
        "            supervised=True,\n",
        "            tasks=sum(tasks[:task_idx + 1], []),\n",
        "            task_idx=task_idx,\n",
        "            buffer_lst=buffer_lst,)\n",
        "        supervised_sampler = ClassStratifiedSampler(\n",
        "            data_source=supervised_set,\n",
        "            world_size=world_size,\n",
        "            rank=rank,\n",
        "            batch_size=s_batch_size,\n",
        "            classes_per_batch=classes_per_batch,\n",
        "            seed=_GLOBAL_SEED)\n",
        "        supervised_loader = torch.utils.data.DataLoader(\n",
        "            supervised_set,\n",
        "            batch_sampler=supervised_sampler,\n",
        "            num_workers=8,\n",
        "            worker_init_fn=seed_worker,\n",
        "            generator=g)\n",
        "        if len(supervised_loader) > 0:\n",
        "            tmp = ceil(len(unsupervised_loader) / len(supervised_loader))\n",
        "            supervised_sampler.set_inner_epochs(tmp)\n",
        "            logger.debug(f'supervised-reset-period {tmp}')\n",
        "\n",
        "    return (unsupervised_loader, unsupervised_sampler,\n",
        "            supervised_loader, supervised_sampler)\n",
        "\n",
        "\n",
        "def _init_imgnt_ft_data(\n",
        "    transform,\n",
        "    init_transform,\n",
        "    batch_size,\n",
        "    stratify=False,\n",
        "    classes_per_batch=1,\n",
        "    unique_classes=False,\n",
        "    world_size=1,\n",
        "    rank=0,\n",
        "    root_path='/datasets/',\n",
        "    image_folder='imagenet_full_size/061417/',\n",
        "    training=True,\n",
        "    copy_data=False,\n",
        "    drop_last=True,\n",
        "    tar_folder='imagenet_full_size/',\n",
        "    tar_file='imagenet_full_size-061417.tar',\n",
        "    tasks=None,\n",
        "    task_idx=0,\n",
        "    visible_class_ul=None\n",
        "):\n",
        "    imagenet = ImageNet(\n",
        "        root=root_path,\n",
        "        image_folder=image_folder,\n",
        "        tar_folder=tar_folder,\n",
        "        tar_file=tar_file,\n",
        "        transform=transform,\n",
        "        train=training,\n",
        "        copy_data=copy_data)\n",
        "    logger.info('ImageNet fine-tune dataset created')\n",
        "    dataset = TransImageNet(\n",
        "        dataset=imagenet,\n",
        "        supervised=True,\n",
        "        init_transform=init_transform,\n",
        "        seed=_GLOBAL_SEED,\n",
        "        tasks=sum(tasks[:task_idx + 1], []))\n",
        "\n",
        "    if not stratify:\n",
        "        dist_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "            dataset=dataset,\n",
        "            num_replicas=world_size,\n",
        "            rank=rank)\n",
        "        data_loader = torch.utils.data.DataLoader(\n",
        "            dataset,\n",
        "            sampler=dist_sampler,\n",
        "            batch_size=batch_size,\n",
        "            drop_last=drop_last,\n",
        "            pin_memory=True,\n",
        "            num_workers=8,\n",
        "            worker_init_fn=seed_worker,\n",
        "            generator=g)\n",
        "    else:\n",
        "        dist_sampler = ClassStratifiedSampler(\n",
        "            data_source=dataset,\n",
        "            world_size=world_size,\n",
        "            rank=rank,\n",
        "            batch_size=batch_size,\n",
        "            classes_per_batch=classes_per_batch,\n",
        "            seed=_GLOBAL_SEED,\n",
        "            unique_classes=unique_classes)\n",
        "        data_loader = torch.utils.data.DataLoader(\n",
        "            dataset,\n",
        "            batch_sampler=dist_sampler,\n",
        "            pin_memory=True,\n",
        "            num_workers=8,\n",
        "            worker_init_fn=seed_worker,\n",
        "            generator=g)\n",
        "\n",
        "    return (data_loader, dist_sampler)\n",
        "\n",
        "\n",
        "def _init_imgnt_data(\n",
        "    transform,\n",
        "    init_transform,\n",
        "    u_batch_size,\n",
        "    s_batch_size,\n",
        "    classes_per_batch,\n",
        "    unique_classes=False,\n",
        "    multicrop_transform=(0, None),\n",
        "    supervised_views=1,\n",
        "    world_size=1,\n",
        "    rank=0,\n",
        "    root_path='/datasets/',\n",
        "    image_folder='imagenet_full_size/061417/',\n",
        "    training=True,\n",
        "    copy_data=False,\n",
        "    tar_folder='imagenet_full_size/',\n",
        "    tar_file='imagenet_full_size-061417.tar',\n",
        "    tasks=None,\n",
        "    task_idx=0,\n",
        "    visible_class_ul=None,\n",
        "    buffer_lst=None\n",
        "):\n",
        "    imagenet = ImageNet(\n",
        "        root=root_path,\n",
        "        image_folder=image_folder,\n",
        "        tar_folder=tar_folder,\n",
        "        tar_file=tar_file,\n",
        "        transform=transform,\n",
        "        train=training,\n",
        "        copy_data=copy_data)\n",
        "    logger.info('ImageNet dataset created')\n",
        "    unsupervised_set = TransImageNet(\n",
        "        dataset=imagenet,\n",
        "        supervised=False,\n",
        "        init_transform=init_transform,\n",
        "        multicrop_transform=multicrop_transform,\n",
        "        seed=_GLOBAL_SEED,\n",
        "        tasks=sum(tasks[:task_idx + 1], []))\n",
        "    if tasks is not None:\n",
        "        if visible_class_ul is None:\n",
        "            visible_class_ul = tasks[task_idx]\n",
        "        indexes = [i for i, c in enumerate(unsupervised_set.targets) if c in visible_class_ul]\n",
        "        # Sample a subset of unlabeled data for the case of current_seen\n",
        "        num_unlab = int(len(indexes)) #/ len(visible_class_ul))\n",
        "        indexes = random.sample(indexes, k=num_unlab)\n",
        "        unsupervised_set = torch.utils.data.Subset(unsupervised_set, indexes)\n",
        "    unsupervised_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "        dataset=unsupervised_set,\n",
        "        num_replicas=world_size,\n",
        "        rank=rank)\n",
        "    unsupervised_loader = torch.utils.data.DataLoader(\n",
        "        unsupervised_set,\n",
        "        sampler=unsupervised_sampler,\n",
        "        batch_size=u_batch_size,\n",
        "        drop_last=True,\n",
        "        pin_memory=True,\n",
        "        num_workers=8,\n",
        "        worker_init_fn=seed_worker,\n",
        "        generator=g)\n",
        "    logger.info('ImageNet unsupervised data loader created')\n",
        "\n",
        "    supervised_sampler, supervised_loader = None, None\n",
        "    if classes_per_batch > 0 and s_batch_size > 0:\n",
        "        logger.info('Making supervised ImageNet data loader...')\n",
        "        supervised_set = TransImageNet(\n",
        "            dataset=imagenet,\n",
        "            supervised=True,\n",
        "            supervised_views=supervised_views,\n",
        "            init_transform=init_transform,\n",
        "            seed=_GLOBAL_SEED,\n",
        "            tasks=sum(tasks[:task_idx + 1], []),\n",
        "            buffer_lst=buffer_lst)\n",
        "        supervised_sampler = ClassStratifiedSampler(\n",
        "            data_source=supervised_set,\n",
        "            world_size=world_size,\n",
        "            rank=rank,\n",
        "            batch_size=s_batch_size,\n",
        "            classes_per_batch=classes_per_batch,\n",
        "            unique_classes=unique_classes,\n",
        "            seed=_GLOBAL_SEED)\n",
        "        supervised_loader = torch.utils.data.DataLoader(\n",
        "            supervised_set,\n",
        "            batch_sampler=supervised_sampler,\n",
        "            pin_memory=True,\n",
        "            num_workers=8,\n",
        "            worker_init_fn=seed_worker,\n",
        "            generator=g)\n",
        "        if len(supervised_loader) > 0:\n",
        "            tmp = ceil(len(unsupervised_loader) / len(supervised_loader))\n",
        "            supervised_sampler.set_inner_epochs(tmp)\n",
        "            logger.info(f'supervised-reset-period {tmp}')\n",
        "        logger.info('ImageNet supervised data loader created')\n",
        "\n",
        "    return (unsupervised_loader, unsupervised_sampler,\n",
        "            supervised_loader, supervised_sampler)\n",
        "\n",
        "\n",
        "def make_transforms(\n",
        "    dataset_name,\n",
        "    subset_path=None,\n",
        "    unlabeled_frac=1.0,\n",
        "    training=True,\n",
        "    basic_augmentations=False,\n",
        "    force_center_crop=False,\n",
        "    crop_scale=(0.08, 1.0),\n",
        "    color_jitter=1.0,\n",
        "    normalize=False,\n",
        "    split_seed=0\n",
        "):\n",
        "    \"\"\"\n",
        "    :param dataset_name: ['imagenet', 'cifar10']\n",
        "    :param subset_path: path to .txt file denoting subset of data to use\n",
        "    :param unlabeled_frac: fraction of data that is unlabeled\n",
        "    :param training: whether to load training data\n",
        "    :param basic_augmentations: whether to use simple data-augmentations\n",
        "    :param force_center_crop: whether to force use of a center-crop\n",
        "    :param color_jitter: strength of color-jitter\n",
        "    :param normalize: whether to normalize color channels\n",
        "    \"\"\"\n",
        "\n",
        "    if 'imagenet' in dataset_name:\n",
        "        logger.info('making imagenet data transforms')\n",
        "\n",
        "        # # -- file identifying which imagenet labels to keep\n",
        "        # keep_file = None\n",
        "        # if subset_path is not None:\n",
        "        #     if unlabeled_frac >= 0:\n",
        "        #         keep_file = os.path.join(subset_path, f'{int(unlabeled_frac* 100)}percent.txt')\n",
        "        #     else:\n",
        "        #         keep_file = os.path.join(subset_path, 'val.txt')\n",
        "        #     logger.info(f'keep file: {keep_file}')\n",
        "\n",
        "        keep_file = subset_path\n",
        "        return _make_imgnt_transforms(\n",
        "            unlabel_prob=unlabeled_frac,\n",
        "            training=training,\n",
        "            basic=basic_augmentations,\n",
        "            force_center_crop=force_center_crop,\n",
        "            normalize=normalize,\n",
        "            color_distortion=color_jitter,\n",
        "            scale=crop_scale,\n",
        "            keep_file=keep_file)\n",
        "\n",
        "    elif 'cifar' in dataset_name:\n",
        "        logger.info('making cifar data transforms')\n",
        "        keep_file = subset_path\n",
        "        logger.info(f'keep file: {keep_file}')\n",
        "\n",
        "        return _make_cifar_transforms(\n",
        "            unlabel_prob=unlabeled_frac,\n",
        "            training=training,\n",
        "            basic=basic_augmentations,\n",
        "            force_center_crop=force_center_crop,\n",
        "            normalize=normalize,\n",
        "            scale=crop_scale,\n",
        "            color_distortion=color_jitter,\n",
        "            keep_file=keep_file)\n",
        "\n",
        "\n",
        "def _make_cifar_transforms(\n",
        "    unlabel_prob,\n",
        "    training=True,\n",
        "    basic=False,\n",
        "    force_center_crop=False,\n",
        "    normalize=False,\n",
        "    scale=(0.5, 1.0),\n",
        "    color_distortion=0.5,\n",
        "    keep_file=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Make data transformations\n",
        "\n",
        "    :param unlabel_prob:probability of sampling unlabeled data point\n",
        "    :param training: generate data transforms for train (alternativly test)\n",
        "    :param basic: whether train transforms include more sofisticated transforms\n",
        "    :param force_center_crop: whether to override settings and apply center crop to image\n",
        "    :param normalize: whether to normalize image means and stds\n",
        "    :param scale: random scaling range for image before resizing\n",
        "    :param color_distortion: strength of color distortion\n",
        "    :param keep_file: file containing names of images to use for semisupervised\n",
        "    \"\"\"\n",
        "    def get_color_distortion(s=1.0):\n",
        "        # s is the strength of color distortion.\n",
        "        color_jitter = transforms.ColorJitter(0.8*s, 0.8*s, 0.8*s, 0.2*s)\n",
        "        rnd_color_jitter = transforms.RandomApply([color_jitter], p=0.8)\n",
        "\n",
        "        color_distort = transforms.Compose([\n",
        "            rnd_color_jitter,\n",
        "            Solarize(p=0.2),\n",
        "            Equalize(p=0.2)])\n",
        "        return color_distort\n",
        "\n",
        "    if training and (not force_center_crop):\n",
        "        if basic:\n",
        "            transform = transforms.Compose(\n",
        "                [transforms.CenterCrop(size=32),\n",
        "                 transforms.RandomHorizontalFlip(),\n",
        "                 transforms.ToTensor()])\n",
        "        else:\n",
        "            transform = transforms.Compose(\n",
        "                [transforms.RandomResizedCrop(size=32, scale=scale),\n",
        "                 transforms.RandomHorizontalFlip(),\n",
        "                 get_color_distortion(s=color_distortion),\n",
        "                 transforms.ToTensor()])\n",
        "    else:\n",
        "        transform = transforms.Compose(\n",
        "            [transforms.CenterCrop(size=32),\n",
        "             transforms.ToTensor()])\n",
        "\n",
        "    if normalize:\n",
        "        transform = transforms.Compose(\n",
        "            [transform,\n",
        "             transforms.Normalize(\n",
        "                 (0.4914, 0.4822, 0.4465),\n",
        "                 (0.2023, 0.1994, 0.2010))])\n",
        "\n",
        "    def init_transform(targets, samples, keep_file=keep_file, training=training, tasks=None, task_idx=None, buffer_lst=None):\n",
        "        \"\"\" Transforms applied to dataset at the start of training \"\"\"\n",
        "        cls_per_task = int(len(tasks) / (task_idx+1))\n",
        "        cur_cls = tasks[-cls_per_task:]\n",
        "        new_targets, new_samples = [], []\n",
        "        if training and (keep_file is not None):\n",
        "            assert os.path.exists(keep_file), 'keep file does not exist'\n",
        "            logger.info(f'Using {keep_file}')\n",
        "            with open(keep_file, 'r') as rfile:\n",
        "                for line in rfile:\n",
        "                    indx = int(line.split('\\n')[0])\n",
        "                    if targets[indx] in cur_cls:\n",
        "                        new_targets.append(targets[indx])\n",
        "                        new_samples.append(samples[indx])\n",
        "                    elif buffer_lst is not None and indx in buffer_lst:\n",
        "                        new_targets.append(targets[indx])\n",
        "                        new_samples.append(samples[indx])\n",
        "\n",
        "        else:\n",
        "            if tasks is not None:\n",
        "                for s, t in zip(samples, targets):\n",
        "                    if t in tasks:\n",
        "                        new_targets.append(t)\n",
        "                        new_samples.append(s)\n",
        "            else:\n",
        "                new_targets, new_samples = targets, samples\n",
        "        return np.array(new_targets), np.array(new_samples)\n",
        "\n",
        "    return transform, init_transform\n",
        "\n",
        "\n",
        "def _make_imgnt_transforms(\n",
        "    unlabel_prob,\n",
        "    training=True,\n",
        "    basic=False,\n",
        "    force_center_crop=False,\n",
        "    normalize=False,\n",
        "    scale=(0.08, 1.0),\n",
        "    color_distortion=1.0,\n",
        "    keep_file=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Make data transformations\n",
        "\n",
        "    :param unlabel_prob: probability of sampling unlabeled data point\n",
        "    :param training: generate data transforms for train (alternativly test)\n",
        "    :param basic: whether train transforms include more sofisticated transforms\n",
        "    :param force_center_crop: whether to override settings and apply center crop to image\n",
        "    :param normalize: whether to normalize image means and stds\n",
        "    :param scale: random scaling range for image before resizing\n",
        "    :param color_distortion: strength of color distortion\n",
        "    :param keep_file: file containing names of images to use for semisupervised\n",
        "    \"\"\"\n",
        "    def get_color_distortion(s=1.0):\n",
        "        # s is the strength of color distortion.\n",
        "        color_jitter = transforms.ColorJitter(0.8*s, 0.8*s, 0.8*s, 0.2*s)\n",
        "        rnd_color_jitter = transforms.RandomApply([color_jitter], p=0.8)\n",
        "        rnd_gray = transforms.RandomGrayscale(p=0.2)\n",
        "        color_distort = transforms.Compose([\n",
        "            rnd_color_jitter,\n",
        "            rnd_gray])\n",
        "        return color_distort\n",
        "\n",
        "    logger.debug(f'uprob: {unlabel_prob}\\t training: {training}\\t basic: {basic}\\t normalize: {normalize}\\t color_distortion: {color_distortion}')\n",
        "    if training and (not force_center_crop):\n",
        "        if basic:\n",
        "            transform = transforms.Compose(\n",
        "                [transforms.RandomResizedCrop(size=224, scale=scale),\n",
        "                 transforms.RandomHorizontalFlip(),\n",
        "                 transforms.ToTensor()])\n",
        "        else:\n",
        "            logger.debug('making training (non-basic) transforms')\n",
        "            transform = transforms.Compose(\n",
        "                [transforms.RandomResizedCrop(size=224, scale=scale),\n",
        "                 transforms.RandomHorizontalFlip(),\n",
        "                 get_color_distortion(s=color_distortion),\n",
        "                 GaussianBlur(p=0.5),\n",
        "                 transforms.ToTensor()])\n",
        "    else:\n",
        "        transform = transforms.Compose(\n",
        "            [transforms.Resize(size=256),\n",
        "             transforms.CenterCrop(size=224),\n",
        "             transforms.ToTensor()])\n",
        "\n",
        "    if normalize:\n",
        "        transform = transforms.Compose(\n",
        "            [transform,\n",
        "             transforms.Normalize(\n",
        "                 (0.485, 0.456, 0.406),\n",
        "                 (0.229, 0.224, 0.225))])\n",
        "\n",
        "\n",
        "    def init_transform(targets, samples, class_to_idx, seed,\n",
        "                       keep_file=keep_file,\n",
        "                       training=training,\n",
        "                       tasks=None,\n",
        "                       task_idx=None,\n",
        "                       buffer_lst=None):\n",
        "        \"\"\" Transforms applied to dataset at the start of training \"\"\"\n",
        "        cls_per_task = int(len(tasks) / (task_idx+1))\n",
        "        cur_cls = tasks[-cls_per_task:]\n",
        "        new_targets, new_samples = [], []\n",
        "        if training and (keep_file is not None) and os.path.exists(keep_file):\n",
        "            logger.info(f'Using {keep_file}')\n",
        "            with open(keep_file, 'r') as rfile:\n",
        "                for line in rfile:\n",
        "                    indx = int(line.split('\\n')[0])\n",
        "                    if targets[indx] in cur_cls:\n",
        "                        new_targets.append(targets[indx])\n",
        "                        new_samples.append(samples[indx])\n",
        "                    elif buffer_lst is not None and indx in buffer_lst:\n",
        "                        new_targets.append(targets[indx])\n",
        "                        new_samples.append(samples[indx])\n",
        "        else:\n",
        "            if tasks is not None:\n",
        "                for s, t in zip(samples, targets):\n",
        "                    if t in tasks:\n",
        "                        new_targets.append(t)\n",
        "                        new_samples.append(s)\n",
        "            else:\n",
        "                new_targets, new_samples = targets, samples\n",
        "\n",
        "        return np.array(new_targets), np.array(new_samples)\n",
        "\n",
        "    return transform, init_transform\n",
        "\n",
        "\n",
        "def make_multicrop_transform(\n",
        "    dataset_name,\n",
        "    num_crops,\n",
        "    size,\n",
        "    crop_scale,\n",
        "    normalize,\n",
        "    color_distortion\n",
        "):\n",
        "    if 'imagenet' in dataset_name:\n",
        "        return _make_multicrop_imgnt_transforms(\n",
        "            num_crops=num_crops,\n",
        "            size=size,\n",
        "            scale=crop_scale,\n",
        "            normalize=normalize,\n",
        "            color_distortion=color_distortion)\n",
        "    elif 'cifar10' in dataset_name:\n",
        "        return _make_multicrop_cifar10_transforms(\n",
        "            num_crops=num_crops,\n",
        "            size=size,\n",
        "            scale=crop_scale,\n",
        "            normalize=normalize,\n",
        "            color_distortion=color_distortion)\n",
        "\n",
        "\n",
        "def _make_multicrop_cifar10_transforms(\n",
        "    num_crops,\n",
        "    size=18,\n",
        "    scale=(0.3, 0.75),\n",
        "    normalize=False,\n",
        "    color_distortion=0.5\n",
        "):\n",
        "\n",
        "    def get_color_distortion(s=1.0):\n",
        "        # s is the strength of color distortion.\n",
        "        color_jitter = transforms.ColorJitter(0.8*s, 0.8*s, 0.8*s, 0.2*s)\n",
        "        rnd_color_jitter = transforms.RandomApply([color_jitter], p=0.8)\n",
        "\n",
        "        color_distort = transforms.Compose([\n",
        "            rnd_color_jitter,\n",
        "            Solarize(p=0.2),\n",
        "            Equalize(p=0.2)])\n",
        "        return color_distort\n",
        "\n",
        "    transform = transforms.Compose(\n",
        "        [transforms.RandomResizedCrop(size=size, scale=scale),\n",
        "         transforms.RandomHorizontalFlip(),\n",
        "         get_color_distortion(s=color_distortion),\n",
        "         transforms.ToTensor()])\n",
        "\n",
        "    if normalize:\n",
        "        transform = transforms.Compose(\n",
        "            [transform,\n",
        "             transforms.Normalize(\n",
        "                 (0.4914, 0.4822, 0.4465),\n",
        "                 (0.2023, 0.1994, 0.2010))])\n",
        "\n",
        "    return (num_crops, transform)\n",
        "\n",
        "\n",
        "def _make_multicrop_imgnt_transforms(\n",
        "    num_crops,\n",
        "    size=96,\n",
        "    scale=(0.05, 0.14),\n",
        "    normalize=False,\n",
        "    color_distortion=1.0,\n",
        "):\n",
        "    def get_color_distortion(s=1.0):\n",
        "        color_jitter = transforms.ColorJitter(0.8*s, 0.8*s, 0.8*s, 0.2*s)\n",
        "        rnd_color_jitter = transforms.RandomApply([color_jitter], p=0.8)\n",
        "        rnd_gray = transforms.RandomGrayscale(p=0.2)\n",
        "        color_distort = transforms.Compose([\n",
        "            rnd_color_jitter,\n",
        "            rnd_gray])\n",
        "        return color_distort\n",
        "\n",
        "    logger.debug('making multicrop transforms')\n",
        "    transform = transforms.Compose(\n",
        "        [transforms.RandomResizedCrop(size=size, scale=scale),\n",
        "         transforms.RandomHorizontalFlip(),\n",
        "         get_color_distortion(s=color_distortion),\n",
        "         GaussianBlur(p=0.5),\n",
        "         transforms.ToTensor()])\n",
        "\n",
        "    if normalize:\n",
        "        transform = transforms.Compose(\n",
        "            [transform,\n",
        "             transforms.Normalize(\n",
        "                 (0.485, 0.456, 0.406),\n",
        "                 (0.229, 0.224, 0.225))])\n",
        "\n",
        "    return (num_crops, transform)\n",
        "\n",
        "\n",
        "class ClassStratifiedSampler(torch.utils.data.Sampler):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        data_source,\n",
        "        world_size,\n",
        "        rank,\n",
        "        batch_size=1,\n",
        "        classes_per_batch=10,\n",
        "        epochs=1,\n",
        "        seed=0,\n",
        "        unique_classes=False\n",
        "    ):\n",
        "        \"\"\"\n",
        "        ClassStratifiedSampler\n",
        "\n",
        "        Batch-sampler that samples 'batch-size' images from subset of randomly\n",
        "        chosen classes e.g., if classes a,b,c are randomly sampled,\n",
        "        the sampler returns\n",
        "            torch.cat([a,b,c], [a,b,c], ..., [a,b,c], dim=0)\n",
        "        where a,b,c, are images from classes a,b,c respectively.\n",
        "        Sampler, samples images WITH REPLACEMENT (i.e., not epoch-based)\n",
        "\n",
        "        :param data_source: dataset of type \"TransImageNet\" or \"TransCIFAR10'\n",
        "        :param world_size: total number of workers in network\n",
        "        :param rank: local rank in network\n",
        "        :param batch_size: num. images to load from each class\n",
        "        :param classes_per_batch: num. classes to randomly sample for batch\n",
        "        :param epochs: num consecutive epochs thru data_source before gen.reset\n",
        "        :param seed: common seed across workers for subsampling classes\n",
        "        :param unique_classes: true ==> each worker samples a distinct set of classes; false ==> all workers sample the same classes\n",
        "        \"\"\"\n",
        "        super(ClassStratifiedSampler, self).__init__(data_source)\n",
        "        self.data_source = data_source\n",
        "\n",
        "        self.rank = rank\n",
        "        self.world_size = world_size\n",
        "        self.cpb = classes_per_batch\n",
        "        self.unique_cpb = unique_classes\n",
        "        self.batch_size = batch_size\n",
        "        self.num_classes = len(set(data_source.targets))\n",
        "        self.classes = set(data_source.targets)\n",
        "        self.epochs = epochs\n",
        "        self.outer_epoch = 0\n",
        "        if not self.unique_cpb:\n",
        "            assert self.num_classes % self.cpb == 0\n",
        "\n",
        "        self.base_seed = seed  # instance seed\n",
        "        self.seed = seed  # subsample sampler seed\n",
        "\n",
        "    def set_epoch(self, epoch):\n",
        "        self.outer_epoch = epoch\n",
        "\n",
        "    def set_inner_epochs(self, epochs):\n",
        "        self.epochs = epochs\n",
        "\n",
        "    def _next_perm(self):\n",
        "        self.seed += 1\n",
        "        g = torch.Generator()\n",
        "        g.manual_seed(self.seed)\n",
        "        self._perm = torch.randperm(self.num_classes, generator=g)\n",
        "\n",
        "    def _get_perm_ssi(self):\n",
        "        start = self._ssi\n",
        "        end = self._ssi + self.cpb\n",
        "        subsample = self._perm[start:end]\n",
        "        return subsample\n",
        "\n",
        "    def _next_ssi(self):\n",
        "        if not self.unique_cpb:\n",
        "            self._ssi = (self._ssi + self.cpb) % self.num_classes\n",
        "            if self._ssi == 0:\n",
        "                self._next_perm()\n",
        "        else:\n",
        "            self._ssi += self.cpb * self.world_size\n",
        "            max_end = self._ssi + self.cpb * (self.world_size - self.rank)\n",
        "            if max_end > self.num_classes:\n",
        "                self._ssi = self.rank * self.cpb\n",
        "                self._next_perm()\n",
        "\n",
        "    def _get_local_samplers(self, epoch):\n",
        "        \"\"\" Generate samplers for local data set in given epoch \"\"\"\n",
        "        seed = int(self.base_seed + epoch\n",
        "                   + self.epochs * self.rank\n",
        "                   + self.outer_epoch * self.epochs * self.world_size)\n",
        "        g = torch.Generator()\n",
        "        g.manual_seed(seed)\n",
        "        samplers = []\n",
        "        for t in self.classes:\n",
        "            t_indices = np.array(self.data_source.target_indices[t])\n",
        "            if not self.unique_cpb:\n",
        "                i_size = len(t_indices) // self.world_size\n",
        "                if i_size > 0:\n",
        "                    t_indices = t_indices[self.rank*i_size:(self.rank+1)*i_size]\n",
        "            if len(t_indices) > 1:\n",
        "                t_indices = t_indices[torch.randperm(len(t_indices), generator=g)]\n",
        "            samplers.append(iter(t_indices))\n",
        "        return samplers\n",
        "\n",
        "    def _subsample_samplers(self, samplers):\n",
        "        \"\"\" Subsample a small set of samplers from all class-samplers \"\"\"\n",
        "        subsample = self._get_perm_ssi()\n",
        "        subsampled_samplers = []\n",
        "        for i in subsample:\n",
        "            subsampled_samplers.append(samplers[i])\n",
        "        self._next_ssi()\n",
        "        return zip(*subsampled_samplers)\n",
        "\n",
        "    def __iter__(self):\n",
        "        self._ssi = self.rank*self.cpb if self.unique_cpb else 0\n",
        "        self._next_perm()\n",
        "\n",
        "        # -- iterations per epoch (extract batch-size samples from each class)\n",
        "        ipe = (self.num_classes // self.cpb if not self.unique_cpb\n",
        "               else self.num_classes // (self.cpb * self.world_size)) * self.batch_size\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "\n",
        "            # -- shuffle class order\n",
        "            samplers = self._get_local_samplers(epoch)\n",
        "            subsampled_samplers = self._subsample_samplers(samplers)\n",
        "\n",
        "            counter, batch = 0, []\n",
        "            for i in range(ipe):\n",
        "                batch += list(next(subsampled_samplers))\n",
        "                counter += 1\n",
        "                if counter == self.batch_size:\n",
        "                    yield batch\n",
        "                    counter, batch = 0, []\n",
        "                    if i + 1 < ipe:\n",
        "                        subsampled_samplers = self._subsample_samplers(samplers)\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.batch_size == 0:\n",
        "            return 0\n",
        "\n",
        "        ipe = (self.num_classes // self.cpb if not self.unique_cpb\n",
        "               else self.num_classes // (self.cpb * self.world_size))\n",
        "        return self.epochs * ipe\n",
        "\n",
        "\n",
        "class ImageNet(torchvision.datasets.ImageFolder):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        root,\n",
        "        image_folder='imagenet_full_size/061417/',\n",
        "        tar_folder='imagenet_full_size/',\n",
        "        tar_file='imagenet_full_size-061417.tar',\n",
        "        train=True,\n",
        "        transform=None,\n",
        "        target_transform=None,\n",
        "        job_id=None,\n",
        "        local_rank=None,\n",
        "        copy_data=True\n",
        "    ):\n",
        "        \"\"\"\n",
        "        ImageNet\n",
        "\n",
        "        Dataset wrapper (can copy data locally to machine)\n",
        "\n",
        "        :param root: root network directory for ImageNet data\n",
        "        :param image_folder: path to images inside root network directory\n",
        "        :param tar_file: zipped image_folder inside root network directory\n",
        "        :param train: whether to load train data (or validation)\n",
        "        :param transform: data-augmentations (applied in data-loader)\n",
        "        :param target_transform: target-transform to apply in data-loader\n",
        "        :param job_id: scheduler job-id used to create dir on local machine\n",
        "        :param copy_data: whether to copy data from network file locally\n",
        "        \"\"\"\n",
        "\n",
        "        suffix = 'train/' if train else 'val/'\n",
        "        data_path = None\n",
        "        if copy_data:\n",
        "            logger.info('copying data locally')\n",
        "            data_path = copy_imgnt_locally(\n",
        "                root=root,\n",
        "                suffix=suffix,\n",
        "                image_folder=image_folder,\n",
        "                tar_folder=tar_folder,\n",
        "                tar_file=tar_file,\n",
        "                job_id=job_id,\n",
        "                local_rank=local_rank)\n",
        "        if (not copy_data) or (data_path is None):\n",
        "            data_path = os.path.join(root, image_folder, suffix)\n",
        "        logger.info(f'data-path {data_path}')\n",
        "\n",
        "        super(ImageNet, self).__init__(\n",
        "            root=data_path,\n",
        "            transform=transform,\n",
        "            target_transform=target_transform)\n",
        "        logger.info('Initialized ImageNet')\n",
        "\n",
        "\n",
        "class TransImageNet(ImageNet):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dataset,\n",
        "        supervised=False,\n",
        "        supervised_views=1,\n",
        "        init_transform=None,\n",
        "        multicrop_transform=(0, None),\n",
        "        seed=0,\n",
        "        task_idx=None,\n",
        "        tasks=None,\n",
        "        buffer_lst=None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        TransImageNet\n",
        "\n",
        "        Dataset that can apply transforms to images on initialization and can\n",
        "        return multiple transformed copies of the same image in each call\n",
        "        to __getitem__\n",
        "        \"\"\"\n",
        "        self.dataset = dataset\n",
        "        self.supervised = supervised\n",
        "        self.supervised_views = supervised_views\n",
        "        self.multicrop_transform = multicrop_transform\n",
        "\n",
        "        self.targets, self.samples = dataset.targets, dataset.samples\n",
        "        \n",
        "        if self.supervised:\n",
        "            self.targets, self.samples = init_transform(\n",
        "                dataset.targets,\n",
        "                dataset.samples,\n",
        "                dataset.class_to_idx,\n",
        "                seed,\n",
        "                task_idx=task_idx,\n",
        "                tasks=tasks,\n",
        "                buffer_lst=buffer_lst)\n",
        "            logger.debug(f'num-labeled {len(self.samples)}')\n",
        "            mint = None\n",
        "            self.target_indices = []\n",
        "            for t in range(len(dataset.classes)):\n",
        "                indices = np.squeeze(np.argwhere(\n",
        "                    self.targets == t)).tolist()\n",
        "                self.target_indices.append(indices)\n",
        "                if isinstance(indices, int):\n",
        "                    indices = [indices]\n",
        "                mint = len(indices) if mint is None else min(mint, len(indices))\n",
        "                logger.info(f'num-labeled target {t} {len(indices)}')\n",
        "            logger.info(f'min. labeled indices {mint}')\n",
        "        \n",
        "    @property\n",
        "    def classes(self):\n",
        "        return self.dataset.classes\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        target = self.targets[index]\n",
        "        path = self.samples[index][0]\n",
        "        img = self.dataset.loader(path)\n",
        "\n",
        "        if self.dataset.target_transform is not None:\n",
        "            target = self.dataset.target_transform(target)\n",
        "\n",
        "        if self.dataset.transform is not None:\n",
        "            if self.supervised:\n",
        "                return *[self.dataset.transform(img) for _ in range(self.supervised_views)], target\n",
        "\n",
        "            else:\n",
        "                img_1 = self.dataset.transform(img)\n",
        "                img_2 = self.dataset.transform(img)\n",
        "\n",
        "                multicrop, mc_transform = self.multicrop_transform\n",
        "                if multicrop > 0 and mc_transform is not None:\n",
        "                    mc_imgs = [mc_transform(img) for _ in range(int(multicrop))]\n",
        "                    return img_1, img_2, *mc_imgs, target\n",
        "\n",
        "                return img_1, img_2, target\n",
        "\n",
        "        return img, target\n",
        "\n",
        "\n",
        "class TransCIFAR10(torchvision.datasets.CIFAR10):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        root,\n",
        "        image_folder='cifar-pytorch/11222017/',\n",
        "        tar_file='cifar-10-python.tar.gz',\n",
        "        copy_data=False,\n",
        "        train=True,\n",
        "        transform=None,\n",
        "        target_transform=None,\n",
        "        init_transform=None,\n",
        "        supervised=True,\n",
        "        multicrop_transform=(0, None),\n",
        "        supervised_views=1,\n",
        "        tasks=None,\n",
        "        task_idx=None,\n",
        "        us=False,\n",
        "        buffer_lst=None,\n",
        "    ):\n",
        "        data_path = None\n",
        "        if copy_data:\n",
        "            logger.info('copying data locally')\n",
        "            data_path = copy_cifar10_locally(\n",
        "                root=root,\n",
        "                image_folder=image_folder,\n",
        "                tar_file=tar_file)\n",
        "        if (not copy_data) or (data_path is None):\n",
        "            data_path = os.path.join(root, image_folder)\n",
        "        logger.info(f'data-path {data_path}')\n",
        "\n",
        "        super().__init__(data_path, train, transform, target_transform, True)\n",
        "        self.training = train\n",
        "        self.supervised_views = supervised_views\n",
        "        self.multicrop_transform = multicrop_transform\n",
        "        self.supervised = supervised\n",
        "        self.us = us\n",
        "        self.not_aug_transform = transforms.Compose([transforms.ToTensor()])\n",
        "        if self.supervised:\n",
        "            self.targets, self.data = init_transform(self.targets, self.data, tasks=tasks, task_idx=task_idx, buffer_lst=buffer_lst)\n",
        "            logger.info(f'num-labeled {len(self.data)}')\n",
        "            mint = None\n",
        "            self.target_indices = []\n",
        "            for t in range(len(self.classes)):\n",
        "                indices = np.squeeze(np.argwhere(self.targets == t)).tolist()\n",
        "                if not isinstance(indices, list):\n",
        "                    indices = [indices]\n",
        "                self.target_indices.append(indices)\n",
        "                mint = len(indices) if mint is None else min(mint, len(indices))\n",
        "                logger.info(f'num-labeled target {t} {len(indices)}')\n",
        "            logger.info(f'min. labeled indices {mint}')\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img, target = self.data[index], self.targets[index]\n",
        "        img = Image.fromarray(img)\n",
        "\n",
        "        if self.target_transform is not None:\n",
        "            target = self.target_transform(target)\n",
        "\n",
        "        if self.transform is not None:\n",
        "\n",
        "            if self.supervised and self.training:\n",
        "                return *[self.transform(img) for _ in range(self.supervised_views)], target\n",
        "                # return self.not_aug_transform(img), target  # Returning un-augmentated images\n",
        "\n",
        "            elif self.supervised and not self.training:\n",
        "                return *[self.transform(img) for _ in range(self.supervised_views)], target\n",
        "            else:\n",
        "                img_1 = self.transform(img)\n",
        "                img_2 = self.transform(img)\n",
        "\n",
        "                multicrop, mc_transform = self.multicrop_transform\n",
        "                if multicrop > 0 and mc_transform is not None:\n",
        "                    mc_imgs = [mc_transform(img) for _ in range(int(multicrop))]\n",
        "                    return img_1, img_2, *mc_imgs, target\n",
        "\n",
        "                return img_1, img_2, target\n",
        "\n",
        "        return img, target\n",
        "\n",
        "\n",
        "class TransCIFAR100(torchvision.datasets.CIFAR100):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        root,\n",
        "        image_folder='cifar-pytorch/11222017/',\n",
        "        tar_file='cifar-100-python.tar.gz',\n",
        "        copy_data=False,\n",
        "        train=True,\n",
        "        transform=None,\n",
        "        target_transform=None,\n",
        "        init_transform=None,\n",
        "        supervised=True,\n",
        "        multicrop_transform=(0, None),\n",
        "        supervised_views=1,\n",
        "        tasks=None,\n",
        "        task_idx=None,\n",
        "        buffer_lst=None,\n",
        "    ):\n",
        "        data_path = None\n",
        "        if copy_data:\n",
        "            logger.info('copying data locally')\n",
        "            data_path = copy_cifar10_locally(\n",
        "                root=root,\n",
        "                image_folder=image_folder,\n",
        "                tar_file=tar_file)\n",
        "        if (not copy_data) or (data_path is None):\n",
        "            data_path = os.path.join(root, image_folder)\n",
        "        logger.info(f'data-path {data_path}')\n",
        "        self.not_aug_transform = transforms.Compose([transforms.ToTensor()])\n",
        "        super().__init__(data_path, train, transform, target_transform, True)\n",
        "\n",
        "        self.supervised_views = supervised_views\n",
        "        self.multicrop_transform = multicrop_transform\n",
        "        self.supervised = supervised\n",
        "        self.training = train\n",
        "        if self.supervised:\n",
        "            self.targets, self.data = init_transform(self.targets, self.data, tasks=tasks, task_idx=task_idx, buffer_lst=buffer_lst)\n",
        "            logger.info(f'num-labeled {len(self.data)}')\n",
        "            mint = None\n",
        "            self.target_indices = []\n",
        "            for t in range(len(self.classes)):\n",
        "                indices = np.squeeze(np.argwhere(self.targets == t)).tolist()\n",
        "                if isinstance(indices, int):\n",
        "                    indices = [indices]\n",
        "                self.target_indices.append(indices)\n",
        "                mint = len(indices) if mint is None else min(mint, len(indices))\n",
        "                logger.info(f'num-labeled target {t} {len(indices)}')\n",
        "            logger.info(f'min. labeled indices {mint}')\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img, target = self.data[index], self.targets[index]\n",
        "        img = Image.fromarray(img)\n",
        "\n",
        "        if self.target_transform is not None:\n",
        "            target = self.target_transform(target)\n",
        "\n",
        "        if self.transform is not None:\n",
        "\n",
        "            if self.supervised and self.training:\n",
        "                return *[self.transform(img) for _ in range(self.supervised_views)], target\n",
        "                # return self.not_aug_transform(img), target # Returning un-augmentated images\n",
        "\n",
        "            elif self.supervised and not self.training:\n",
        "                return *[self.transform(img) for _ in range(self.supervised_views)], target\n",
        "\n",
        "            else:\n",
        "                img_1 = self.transform(img)\n",
        "                img_2 = self.transform(img)\n",
        "\n",
        "                multicrop, mc_transform = self.multicrop_transform\n",
        "                if multicrop > 0 and mc_transform is not None:\n",
        "                    mc_imgs = [mc_transform(img) for _ in range(int(multicrop))]\n",
        "                    return img_1, img_2, *mc_imgs, target\n",
        "\n",
        "                return img_1, img_2, target\n",
        "\n",
        "        return img, target\n",
        "\n",
        "\n",
        "def copy_imgnt_locally(\n",
        "    root,\n",
        "    suffix,\n",
        "    image_folder='imagenet_full_size/061417/',\n",
        "    tar_folder='imagenet_full_size/',\n",
        "    tar_file='imagenet_full_size-061417.tar',\n",
        "    job_id=None,\n",
        "    local_rank=None\n",
        "):\n",
        "    if job_id is None:\n",
        "        try:\n",
        "            job_id = os.environ['SLURM_JOBID']\n",
        "        except Exception:\n",
        "            logger.info('No job-id, will load directly from network file')\n",
        "            return None\n",
        "\n",
        "    if local_rank is None:\n",
        "        try:\n",
        "            local_rank = int(os.environ['SLURM_LOCALID'])\n",
        "        except Exception:\n",
        "            logger.info('No job-id, will load directly from network file')\n",
        "            return None\n",
        "\n",
        "    source_file = os.path.join(root, tar_folder, tar_file)\n",
        "    target = f'/scratch/slurm_tmpdir/{job_id}/'\n",
        "    target_file = os.path.join(target, tar_file)\n",
        "    data_path = os.path.join(target, image_folder, suffix)\n",
        "    logger.info(f'{source_file}\\n{target}\\n{target_file}\\n{data_path}')\n",
        "\n",
        "    tmp_sgnl_file = os.path.join(target, 'copy_signal.txt')\n",
        "\n",
        "    if not os.path.exists(data_path):\n",
        "        if local_rank == 0:\n",
        "            commands = [\n",
        "                ['tar', '-xf', source_file, '-C', target]]\n",
        "            for cmnd in commands:\n",
        "                start_time = time.time()\n",
        "                logger.info(f'Executing {cmnd}')\n",
        "                subprocess.run(cmnd)\n",
        "                logger.info(f'Cmnd took {(time.time()-start_time)/60.} min.')\n",
        "            with open(tmp_sgnl_file, '+w') as f:\n",
        "                print('Done copying locally.', file=f)\n",
        "        else:\n",
        "            while not os.path.exists(tmp_sgnl_file):\n",
        "                time.sleep(60)\n",
        "                logger.info(f'{local_rank}: Checking {tmp_sgnl_file}')\n",
        "\n",
        "    return data_path\n",
        "    \n",
        "\n",
        "def copy_cifar10_locally(\n",
        "    root,\n",
        "    image_folder='cifar-pytorch/11222017/',\n",
        "    tar_file='cifar-10-python.tar.gz',\n",
        "    job_id=None,\n",
        "    local_rank=None\n",
        "):\n",
        "    if job_id is None:\n",
        "        try:\n",
        "            job_id = os.environ['SLURM_JOBID']\n",
        "        except Exception:\n",
        "            logger.info('No job-id, will load directly from network file')\n",
        "            return None\n",
        "\n",
        "    if local_rank is None:\n",
        "        try:\n",
        "            local_rank = int(os.environ['SLURM_LOCALID'])\n",
        "        except Exception:\n",
        "            logger.info('No job-id, will load directly from network file')\n",
        "            return None\n",
        "\n",
        "    source_file = os.path.join(root, image_folder, tar_file)\n",
        "    target = f'/scratch/slurm_tmpdir/{job_id}/'\n",
        "    target_file = os.path.join(target, tar_file)\n",
        "    data_path = target\n",
        "    logger.info(f'{source_file}\\n{target}\\n{target_file}\\n{data_path}')\n",
        "\n",
        "    tmp_sgnl_file = os.path.join(target, 'copy_signal.txt')\n",
        "\n",
        "    if not os.path.exists(tmp_sgnl_file):\n",
        "        if local_rank == 0:\n",
        "            commands = [\n",
        "                ['tar', '-xf', source_file, '-C', target]]\n",
        "            for cmnd in commands:\n",
        "                start_time = time.time()\n",
        "                logger.info(f'Executing {cmnd}')\n",
        "                subprocess.run(cmnd)\n",
        "                logger.info(f'Cmnd took {(time.time()-start_time)/60.} min.')\n",
        "            with open(tmp_sgnl_file, '+w') as f:\n",
        "                print('Done copying locally.', file=f)\n",
        "        else:\n",
        "            while not os.path.exists(tmp_sgnl_file):\n",
        "                time.sleep(60)\n",
        "                logger.info(f'{local_rank}: Checking {tmp_sgnl_file}')\n",
        "\n",
        "    return data_path\n",
        "\n",
        "\n",
        "class Solarize(object):\n",
        "    def __init__(self, p=0.2):\n",
        "        self.prob = p\n",
        "\n",
        "    def __call__(self, img):\n",
        "        if torch.bernoulli(torch.tensor(self.prob)) == 0:\n",
        "            return img\n",
        "\n",
        "        v = torch.rand(1) * 256\n",
        "        return ImageOps.solarize(img, v)\n",
        "\n",
        "\n",
        "class Equalize(object):\n",
        "    def __init__(self, p=0.2):\n",
        "        self.prob = p\n",
        "\n",
        "    def __call__(self, img):\n",
        "        if torch.bernoulli(torch.tensor(self.prob)) == 0:\n",
        "            return img\n",
        "\n",
        "        return ImageOps.equalize(img)\n",
        "\n",
        "\n",
        "class GaussianBlur(object):\n",
        "    def __init__(self, p=0.5, radius_min=0.1, radius_max=2.):\n",
        "        self.prob = p\n",
        "        self.radius_min = radius_min\n",
        "        self.radius_max = radius_max\n",
        "\n",
        "    def __call__(self, img):\n",
        "        if torch.bernoulli(torch.tensor(self.prob)) == 0:\n",
        "            return img\n",
        "\n",
        "        radius = self.radius_min + torch.rand(1) * (self.radius_max - self.radius_min)\n",
        "        return img.filter(ImageFilter.GaussianBlur(radius=radius))\n",
        "\n",
        "def construct_cifar10_subsets(targets, num_samples=50):\n",
        "    import random \n",
        "    random.seed(_GLOBAL_SEED)\n",
        "    idx_dict = {}\n",
        "    for i, val in enumerate(targets):\n",
        "        if val not in idx_dict.keys():\n",
        "            idx_dict[val] = []\n",
        "        idx_dict[val].append(i)\n",
        "    with open('./subsets/cifar10/spc.{}_split.{}.txt'.format(num_samples*10, _GLOBAL_SEED),'w') as file:\n",
        "        for i in range(10):\n",
        "            print(idx_dict[i][-30:])\n",
        "            idx_lst = random.sample(idx_dict[i], num_samples)\n",
        "            for idx in idx_lst:\n",
        "                file.write(str(idx))\n",
        "                file.write('\\n')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "u_batch_size = 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 's_batch_size' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\hp\\OneDrive\\Documents\\GitHub\\NNCSL\\encoder.ipynb Cell 11\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hp/OneDrive/Documents/GitHub/NNCSL/encoder.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m (unsupervised_loader,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hp/OneDrive/Documents/GitHub/NNCSL/encoder.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m unsupervised_sampler,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hp/OneDrive/Documents/GitHub/NNCSL/encoder.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m supervised_loader,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hp/OneDrive/Documents/GitHub/NNCSL/encoder.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m supervised_sampler) \u001b[39m=\u001b[39m_init_cifar10_data(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hp/OneDrive/Documents/GitHub/NNCSL/encoder.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                         transform\u001b[39m=\u001b[39mtransform,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hp/OneDrive/Documents/GitHub/NNCSL/encoder.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m                         init_transform\u001b[39m=\u001b[39minit_transform,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hp/OneDrive/Documents/GitHub/NNCSL/encoder.ipynb#X12sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m                         u_batch_size\u001b[39m=\u001b[39mu_batch_size,\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/hp/OneDrive/Documents/GitHub/NNCSL/encoder.ipynb#X12sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m                         s_batch_size\u001b[39m=\u001b[39ms_batch_size,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hp/OneDrive/Documents/GitHub/NNCSL/encoder.ipynb#X12sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m                         classes_per_batch\u001b[39m=\u001b[39mclasses_per_batch,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hp/OneDrive/Documents/GitHub/NNCSL/encoder.ipynb#X12sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m                         multicrop_transform\u001b[39m=\u001b[39mmulticrop_transform,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hp/OneDrive/Documents/GitHub/NNCSL/encoder.ipynb#X12sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m                         supervised_views\u001b[39m=\u001b[39msupervised_views,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hp/OneDrive/Documents/GitHub/NNCSL/encoder.ipynb#X12sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m                         world_size\u001b[39m=\u001b[39mworld_size,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hp/OneDrive/Documents/GitHub/NNCSL/encoder.ipynb#X12sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m                         rank\u001b[39m=\u001b[39mrank,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hp/OneDrive/Documents/GitHub/NNCSL/encoder.ipynb#X12sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m                         root_path\u001b[39m=\u001b[39mroot_path,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hp/OneDrive/Documents/GitHub/NNCSL/encoder.ipynb#X12sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m                         image_folder\u001b[39m=\u001b[39mimage_folder,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hp/OneDrive/Documents/GitHub/NNCSL/encoder.ipynb#X12sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m                         training\u001b[39m=\u001b[39mtraining,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hp/OneDrive/Documents/GitHub/NNCSL/encoder.ipynb#X12sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m                         copy_data\u001b[39m=\u001b[39mcopy_data,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hp/OneDrive/Documents/GitHub/NNCSL/encoder.ipynb#X12sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m                         tasks\u001b[39m=\u001b[39mtasks,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hp/OneDrive/Documents/GitHub/NNCSL/encoder.ipynb#X12sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m                         task_idx\u001b[39m=\u001b[39mtask_idx,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hp/OneDrive/Documents/GitHub/NNCSL/encoder.ipynb#X12sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m                         visible_class_ul\u001b[39m=\u001b[39mvisible_class_ul,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hp/OneDrive/Documents/GitHub/NNCSL/encoder.ipynb#X12sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m                         us\u001b[39m=\u001b[39mus,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hp/OneDrive/Documents/GitHub/NNCSL/encoder.ipynb#X12sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m                         buffer_lst\u001b[39m=\u001b[39mbuffer_lst)\n",
            "\u001b[1;31mNameError\u001b[0m: name 's_batch_size' is not defined"
          ]
        }
      ],
      "source": [
        "(unsupervised_loader,\n",
        "unsupervised_sampler,\n",
        "supervised_loader,\n",
        "supervised_sampler) =_init_cifar10_data(\n",
        "                        transform=transform,\n",
        "                        init_transform=init_transform,\n",
        "                        u_batch_size=u_batch_size,\n",
        "                        s_batch_size=s_batch_size,\n",
        "                        classes_per_batch=classes_per_batch,\n",
        "                        multicrop_transform=multicrop_transform,\n",
        "                        supervised_views=supervised_views,\n",
        "                        world_size=world_size,\n",
        "                        rank=rank,\n",
        "                        root_path=root_path,\n",
        "                        image_folder=image_folder,\n",
        "                        training=training,\n",
        "                        copy_data=copy_data,\n",
        "                        tasks=tasks,\n",
        "                        task_idx=task_idx,\n",
        "                        visible_class_ul=visible_class_ul,\n",
        "                        us=us,\n",
        "                        buffer_lst=buffer_lst)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
